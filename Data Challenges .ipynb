{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d709f997",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcedad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12c7bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'E:/python/Dataset/OnlineNewsPopularity.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65a8cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 61 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   url                            39644 non-null  object \n",
      " 1   timedelta                      39644 non-null  int64  \n",
      " 2   n_tokens_title                 39644 non-null  int64  \n",
      " 3   tokens_content                 39644 non-null  int64  \n",
      " 4   n_unique_tokens                39644 non-null  float64\n",
      " 5   n_non_stop_words               39644 non-null  float64\n",
      " 6   n_non_stop_unique_tokens       39644 non-null  float64\n",
      " 7   num_hrefs                      39644 non-null  int64  \n",
      " 8   num_self_hrefs                 39644 non-null  int64  \n",
      " 9   images_content                 39644 non-null  int64  \n",
      " 10  videos_content                 39644 non-null  int64  \n",
      " 11  average_token_length           39644 non-null  float64\n",
      " 12  keywords_content               39644 non-null  int64  \n",
      " 13  data_channel_is_lifestyle      39644 non-null  int64  \n",
      " 14  data_channel_is_entertainment  39644 non-null  int64  \n",
      " 15  data_channel_is_bus            39644 non-null  int64  \n",
      " 16  data_channel_is_socmed         39644 non-null  int64  \n",
      " 17  data_channel_is_tech           39644 non-null  int64  \n",
      " 18  data_channel_is_world          39644 non-null  int64  \n",
      " 19  kw_min_min                     39644 non-null  int64  \n",
      " 20  kw_max_min                     39644 non-null  float64\n",
      " 21  kw_avg_min                     39644 non-null  float64\n",
      " 22  kw_min_max                     39644 non-null  int64  \n",
      " 23  kw_max_max                     39644 non-null  int64  \n",
      " 24  kw_avg_max                     39644 non-null  float64\n",
      " 25  kw_min_avg                     39644 non-null  float64\n",
      " 26  kw_max_avg                     39644 non-null  float64\n",
      " 27  kw_avg_avg                     39644 non-null  float64\n",
      " 28  self_reference_min_shares      39644 non-null  float64\n",
      " 29  self_reference_max_shares      39644 non-null  float64\n",
      " 30  self_reference_avg_sharess     39644 non-null  float64\n",
      " 31  weekday_is_monday              39644 non-null  int64  \n",
      " 32  weekday_is_tuesday             39644 non-null  int64  \n",
      " 33  weekday_is_wednesday           39644 non-null  int64  \n",
      " 34  weekday_is_thursday            39644 non-null  int64  \n",
      " 35  weekday_is_friday              39644 non-null  int64  \n",
      " 36  weekday_is_saturday            39644 non-null  int64  \n",
      " 37  weekday_is_sunday              39644 non-null  int64  \n",
      " 38  is_weekend                     39644 non-null  int64  \n",
      " 39  LDA_00                         39644 non-null  float64\n",
      " 40  LDA_01                         39644 non-null  float64\n",
      " 41  LDA_02                         39644 non-null  float64\n",
      " 42  LDA_03                         39644 non-null  float64\n",
      " 43  LDA_04                         39644 non-null  float64\n",
      " 44  global_subjectivity            39644 non-null  float64\n",
      " 45  global_sentiment_polarity      39644 non-null  float64\n",
      " 46  global_rate_positive_words     39644 non-null  float64\n",
      " 47  global_rate_negative_words     39644 non-null  float64\n",
      " 48  rate_positive_words            39644 non-null  float64\n",
      " 49  rate_negative_words            39644 non-null  float64\n",
      " 50  avg_positive_polarity          39644 non-null  float64\n",
      " 51  min_positive_polarity          39644 non-null  float64\n",
      " 52  max_positive_polarity          39644 non-null  float64\n",
      " 53  avg_negative_polarity          39644 non-null  float64\n",
      " 54  min_negative_polarity          39644 non-null  float64\n",
      " 55  max_negative_polarity          39644 non-null  float64\n",
      " 56  title_subjectivity             39644 non-null  float64\n",
      " 57  title_sentiment_polarity       39644 non-null  float64\n",
      " 58  abs_title_subjectivity         39644 non-null  float64\n",
      " 59  abs_title_sentiment_polarity   39644 non-null  float64\n",
      " 60  shares                         39644 non-null  int64  \n",
      "dtypes: float64(34), int64(26), object(1)\n",
      "memory usage: 18.5+ MB\n",
      "Dataset Info:\n",
      " None\n",
      "\n",
      "Summary Statistics:\n",
      "           timedelta  n_tokens_title  tokens_content  n_unique_tokens   \n",
      "count  39644.000000    39644.000000    39644.000000     39644.000000  \\\n",
      "mean     354.530471       10.398749      546.514731         0.548216   \n",
      "std      214.163767        2.114037      471.107508         3.520708   \n",
      "min        8.000000        2.000000        0.000000         0.000000   \n",
      "25%      164.000000        9.000000      246.000000         0.470870   \n",
      "50%      339.000000       10.000000      409.000000         0.539226   \n",
      "75%      542.000000       12.000000      716.000000         0.608696   \n",
      "max      731.000000       23.000000     8474.000000       701.000000   \n",
      "\n",
      "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs   \n",
      "count      39644.000000              39644.000000  39644.000000  \\\n",
      "mean           0.996469                  0.689175     10.883690   \n",
      "std            5.231231                  3.264816     11.332017   \n",
      "min            0.000000                  0.000000      0.000000   \n",
      "25%            1.000000                  0.625739      4.000000   \n",
      "50%            1.000000                  0.690476      8.000000   \n",
      "75%            1.000000                  0.754630     14.000000   \n",
      "max         1042.000000                650.000000    304.000000   \n",
      "\n",
      "       num_self_hrefs  images_content  videos_content  ...   \n",
      "count    39644.000000    39644.000000    39644.000000  ...  \\\n",
      "mean         3.293638        4.544143        1.249874  ...   \n",
      "std          3.855141        8.309434        4.107855  ...   \n",
      "min          0.000000        0.000000        0.000000  ...   \n",
      "25%          1.000000        1.000000        0.000000  ...   \n",
      "50%          3.000000        1.000000        0.000000  ...   \n",
      "75%          4.000000        4.000000        1.000000  ...   \n",
      "max        116.000000      128.000000       91.000000  ...   \n",
      "\n",
      "       min_positive_polarity  max_positive_polarity  avg_negative_polarity   \n",
      "count           39644.000000           39644.000000           39644.000000  \\\n",
      "mean                0.095446               0.756728              -0.259524   \n",
      "std                 0.071315               0.247786               0.127726   \n",
      "min                 0.000000               0.000000              -1.000000   \n",
      "25%                 0.050000               0.600000              -0.328383   \n",
      "50%                 0.100000               0.800000              -0.253333   \n",
      "75%                 0.100000               1.000000              -0.186905   \n",
      "max                 1.000000               1.000000               0.000000   \n",
      "\n",
      "       min_negative_polarity  max_negative_polarity  title_subjectivity   \n",
      "count           39644.000000           39644.000000        39644.000000  \\\n",
      "mean               -0.521944              -0.107500            0.282353   \n",
      "std                 0.290290               0.095373            0.324247   \n",
      "min                -1.000000              -1.000000            0.000000   \n",
      "25%                -0.700000              -0.125000            0.000000   \n",
      "50%                -0.500000              -0.100000            0.150000   \n",
      "75%                -0.300000              -0.050000            0.500000   \n",
      "max                 0.000000               0.000000            1.000000   \n",
      "\n",
      "       title_sentiment_polarity  abs_title_subjectivity   \n",
      "count              39644.000000            39644.000000  \\\n",
      "mean                   0.071425                0.341843   \n",
      "std                    0.265450                0.188791   \n",
      "min                   -1.000000                0.000000   \n",
      "25%                    0.000000                0.166667   \n",
      "50%                    0.000000                0.500000   \n",
      "75%                    0.150000                0.500000   \n",
      "max                    1.000000                0.500000   \n",
      "\n",
      "       abs_title_sentiment_polarity         shares  \n",
      "count                  39644.000000   39644.000000  \n",
      "mean                       0.156064    3395.380184  \n",
      "std                        0.226294   11626.950749  \n",
      "min                        0.000000       1.000000  \n",
      "25%                        0.000000     946.000000  \n",
      "50%                        0.000000    1400.000000  \n",
      "75%                        0.250000    2800.000000  \n",
      "max                        1.000000  843300.000000  \n",
      "\n",
      "[8 rows x 60 columns]\n",
      "\n",
      "Missing Values:\n",
      " url                             0\n",
      "timedelta                       0\n",
      "n_tokens_title                  0\n",
      "tokens_content                  0\n",
      "n_unique_tokens                 0\n",
      "                               ..\n",
      "title_subjectivity              0\n",
      "title_sentiment_polarity        0\n",
      "abs_title_subjectivity          0\n",
      "abs_title_sentiment_polarity    0\n",
      "shares                          0\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about the dataset\n",
    "print(\"Dataset Info:\\n\", df.info())\n",
    "print(\"\\nSummary Statistics:\\n\", df.describe())\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b0d0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction from URL\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return pd.Series({\n",
    "        'domain': parsed_url.netloc,\n",
    "        'url_length': len(url),\n",
    "        'num_subdirectories': url.count('/')\n",
    "    })\n",
    "\n",
    "df[['domain', 'url_length', 'num_subdirectories']] = df['url'].apply(extract_url_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8002b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping: Extract Title or Meta Description from URL (Optional & Time-Consuming)\n",
    "def fetch_page_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No Title\"\n",
    "        return title\n",
    "    except:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "433a0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to scrape titles (Warning: This is slow!)\n",
    "# df['page_title'] = df['url'].apply(fetch_page_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd321056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Engineering: Creating additional features\n",
    "df['content_length_category'] = pd.cut(df['tokens_content'], bins=[0, 300, 600, np.inf], labels=['Short', 'Medium', 'Long'])\n",
    "df['num_media'] = df['images_content'] + df['videos_content']  # Total media count\n",
    "df['keyword_density'] = df['keywords_content'] / (df['tokens_content'] + 1)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b1d3e",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e40614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Engineering: Creating additional features\n",
    "# columns_to_check = ['n_tokens_content', 'num_imgs', 'num_videos', 'num_keywords']\n",
    "# missing_columns = [col for col in columns_to_check if col not in df.columns]\n",
    "\n",
    "# columns_to_check\n",
    "# missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18220644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not missing_columns:\n",
    "#     df['content_length_category'] = pd.cut(df['n_tokens_content'], bins=[0, 300, 600, np.inf], labels=['Short', 'Medium', 'Long'])\n",
    "#     df['num_media'] = df['num_imgs'] + df['num_videos']  # Total media count\n",
    "#     df['keyword_density'] = df['num_keywords'] / (df['n_tokens_content'] + 1)  # Avoid division by zero\n",
    "# else:\n",
    "#     print(f\"Warning: Missing columns in dataset - {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cec741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text-based Feature Extraction: Sentiment Analysis\n",
    "# def analyze_sentiment(text):\n",
    "#     blob = TextBlob(str(text))\n",
    "#     return pd.Series({\n",
    "#         'text_sentiment_polarity': blob.sentiment.polarity,\n",
    "#         'text_subjectivity': blob.sentiment.subjectivity\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6e53938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'title' column not found, skipping sentiment analysis.\n"
     ]
    }
   ],
   "source": [
    "# # Apply sentiment analysis if 'title' column exists\n",
    "# if 'title' in df.columns:\n",
    "#     df[['text_sentiment_polarity', 'text_subjectivity']] = df['title'].apply(analyze_sentiment)\n",
    "# else:\n",
    "#     print(\"Warning: 'title' column not found, skipping sentiment analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb20955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
